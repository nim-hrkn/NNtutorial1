{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd49452c-259f-4df3-bf74-a05dc3fafd80",
   "metadata": {},
   "source": [
    "上記のPythonコードによるDiffusion Modelを、Variational Autoencoder（VAE）との比較に倣って明確に整理して解説します。\n",
    "\n",
    "---\n",
    "\n",
    "## 【1】Encoder（forward 拡散プロセス）について\n",
    "\n",
    "このDiffusion ModelにおけるEncoderは、データに徐々にノイズを付加するプロセスで、次式により表されます：\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)\n",
    "$$\n",
    "\n",
    "さらに、このコードでは **コサインスケジュール（cosine_beta_schedule）** によって $\\beta_t$ を決定します。\n",
    "\n",
    "各ステップの潜在状態 $ x_t $ は、初期状態 $ x_0 $ を用いて直接以下の式でサンプリングされます：\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t)I),\\quad\\text{where}\\quad \\bar{\\alpha}_t=\\prod_{s=1}^{t}(1-\\beta_s)\n",
    "$$\n",
    "\n",
    "具体的にコード中では：\n",
    "\n",
    "```python\n",
    "sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "```\n",
    "\n",
    "と明示されており、これは明らかにVAEのEncoderとは異なり：\n",
    "\n",
    "- **学習パラメータを持たない**\n",
    "- **平均と標準偏差は予め設定され固定**  \n",
    "\n",
    "となります。\n",
    "\n",
    "---\n",
    "\n",
    "## 【2】Decoder（逆拡散プロセス）について\n",
    "\n",
    "Decoderは逆方向のノイズ除去プロセスであり、ニューラルネットワーク（CNN）で表現されます：\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x_{t-1}\\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t,t), \\sigma_t^2I)\n",
    "$$\n",
    "\n",
    "ここで、\n",
    "\n",
    "- 平均 $\\mu_\\theta(x_t, t)$ は、CNNモデルが予測するものに該当します。\n",
    "- このコードのCNN（`CNN_DiffusionModel`）は入力として画像 $x_t$ と時刻情報 $t$ を受け取り、ノイズを直接予測する形式になっています。\n",
    "- **分散（標準偏差）$\\sigma_t$** はコード内で明示的に学習されておらず、一般的な実装に倣いここでも固定されています。\n",
    "\n",
    "具体的にCNNモデルは以下の通りです：\n",
    "\n",
    "```python\n",
    "self.conv = nn.Sequential(\n",
    "    nn.Conv2d(input_channels + 1, hidden_dim, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(hidden_dim, input_channels, kernel_size=3, padding=1)\n",
    ")\n",
    "```\n",
    "\n",
    "時刻情報を考慮するために時刻埋め込み（`TimeEmbedding`）も使用しています：\n",
    "\n",
    "```python\n",
    "t_emb = self.time_embed(t)\n",
    "t_feat = self.fc_time(t_emb).view(-1, 1, 8, 8)\n",
    "```\n",
    "\n",
    "これらを画像のチャネルとしてCNNに渡します。\n",
    "\n",
    "---\n",
    "\n",
    "## 【3】VAEとの比較（まとめ）\n",
    "\n",
    "明確に比較すると以下のようになります：\n",
    "\n",
    "| モデルの要素 | Variational Autoencoder | このDiffusion Model |\n",
    "|---|---|---|\n",
    "| Encoder | NNにより平均・標準偏差を推定する（学習）| コサインスケジュールに基づいて平均・分散は固定（学習なし）|\n",
    "| Decoder | NNにより平均・標準偏差を推定する（学習）| NN(CNN)によりノイズを予測（平均のみ学習）、標準偏差は固定 |\n",
    "| 潜在変数 | 単一のベクトル$ z $ | 時刻ごとの画像$ x_{1:T} $ |\n",
    "| 再パラメータ化トリック | 使用 | 使用 |\n",
    "\n",
    "- このDiffusion Modelは、標準的なDDPMの枠組みに沿っており、VAEと異なり「潜在空間」を逐次的な複数の画像に設定しています。\n",
    "- Encoderは学習対象ではなく、Decoder（逆拡散）のみがCNNで学習対象となります。\n",
    "\n",
    "---\n",
    "\n",
    "## 【4】損失関数（学習目標）\n",
    "\n",
    "損失関数はMSEで、CNNが「付加されたノイズ」を直接予測できるように学習されます：\n",
    "\n",
    "```python\n",
    "loss = mse_loss(noise_pred, noise.to(device))\n",
    "```\n",
    "\n",
    "これは、Diffusion Modelでよく採用される学習方法であり、VAEが「元画像の再構成」を学習するのに対して、「ノイズの再構成」を学習しています。\n",
    "\n",
    "---\n",
    "\n",
    "## 【5】まとめ\n",
    "\n",
    "以上を整理すると、このDiffusion Modelは：\n",
    "\n",
    "- Encoder（forward拡散）は**パラメータ化されず固定のノイズスケジュール**を用いる。\n",
    "- Decoder（逆拡散）は**CNNモデルでノイズの予測を行う**（平均のみ予測）。\n",
    "- 標準偏差は固定（学習しない）。\n",
    "- VAEのように潜在空間が単一ベクトルではなく、逐次的なノイズ画像列で構成される。\n",
    "\n",
    "という特徴を持っています。\n",
    "\n",
    "このように、Diffusion ModelはVAEを拡張し、潜在変数をマルコフ的な多段階に置き換えたモデルと捉えることができます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
